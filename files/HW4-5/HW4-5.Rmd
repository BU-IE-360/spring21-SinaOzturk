---
title: "HW 4&5"
author: "Group 5: Yusuf Sina Ozturk - Ahmet Bugra Taksuk - Ahmet Tabakoglu"
date: "Last edited `r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, libraries, include=F}
library(tidyverse)
library(lubridate)
library(zoo)
library(ggplot2)
library(data.table)
library(dplyr)
library(forecast)
library(miscTools)
library(stats)
library(GGally)
library("readxl")
library(fpp)
library(tseries)
```

- In this homework, we are asked to develop alternative forecasting strategies for our project data which is the sales data and the related information of nine products of Trendyol at different categories. I'm going to investigate the seasonality for each product at different levels such as daily, weekly or monthly. Then I am going to fit the data into ARIMA model based on my findings about seasonality. Lastly, in the third and in the last task, I will check for external regressors and whether I can use them in my model or not.


## PRODUCT ID: 48740784

### TASK 1 - Analyzing Seasonality

### 1 ) Importing and Manipulating the data 

```{r}
pr1 = read.csv("alldata_item1.csv")
pr1 <- as.data.table(pr1)
pr1 <- pr1[,-c("X","w_day")]
```

```{r,warning=F}
pr1 <- mutate(pr1, event_date = mdy(event_date)) # converting event date into datetime object
pr1[, Month:=as.numeric(lubridate::month(event_date,label=F))] #adding month information as a numeric variable 
pr1[, Day:=as.numeric(lubridate::wday(event_date,label=F))] #adding day information as a numeric variable 
head(pr1)
```

Because there is a lot NA's for sold amount of the product, I dropped these rows.

```{r}
pr1 <- drop_na(pr1)
```

However, we are going to build arima models, we should have continuous time series. Therefore, I have to take only the last part of data set.

```{r}
pr1 <- pr1[c(1:41),]
```

Now, we have very small data set. 

For time series analysis, I only deal with the sold amount of the product so I drop possible regressors

```{r,fig.width=10}
sold <- data.table(event_date =pr1$event_date,
                   sold_count = pr1$sold_count)
head(sold)
``` 
### Visualizations

To detect outliers, first I draw boxplot

```{r,fig.width=10}
boxplot(sold$sold_count)
```

```{r,fig.width=10}
ggplot(sold, aes(x = event_date)) + 
  geom_line(aes(y = sold_count)) +  ggtitle("Product 48740784 Sold Amount") +
  xlab("Date") + ylab("Amount Sold")

```
#### ACF Plot

```{r,fig.width=10}
acf(sold$sold_count)
```

#### PACF Plot

```{r,fig.width=10}
pacf(sold$sold_count)
```

### 3 ) Decomposing the data

#### Weekly Seasonality

Actually, we only have 41 data points and we onyl check weekly seasonality.

```{r,fig.width=10}
soldts <- ts(rev(pr1$sold_count),  freq = 7, start= c(1,1))
resultweekdec <- decompose(soldts,type= "additive")
plot(resultweekdec)
```

#### Random term after decomposing weekly

```{r,fig.width=10}
plot(resultweekdec$random)
title("Random Term of Weekly Decomposed Data")
```

To built for this product, there is no chance to built a model using different seasonality. Therefore, I have to use weeekly seasonality.

```{r,fig.width=10}
random = resultweekdec$random
trend = resultweekdec$trend
season = resultweekdec$seasonal
```

### Task 2- Fitting data into ARIMA Model

#### 1 ) Deciding on the parameters

```{r,fig.width=10}
acf(random, na.action = na.pass)
```

```{r,fig.width=10}
pacf(random, na.action = na.pass)
```

From acf and pacf plots, the random part is already stationary. We can use auto.arima function to find best model.

```{r,fig.width=10}
model <- auto.arima(random)
model
```

The best model was found by auto.arima with p=1 and P =1 SARIMA model with season frequency 7.

#### 2 ) Fitting the Model

```{r,fig.width=10}
plot(model$residuals)
title("Residuals")
```

```{r,fig.width=10}
modelfit <- random - model$residuals
fitted <- modelfit+trend+season

plot(soldts)
points(fitted, type= "l", col = 2)

```

#### Comments

- For that much small data set, the model fitted very well. Let's try to add regressors into model to develop the model. 

### Task 3 and 4 - Adding external regressors to the model

#### 1 ) Searcing for regressors

- In this part, I am going to look for variables that might be correlated to sold amount and can be used as regressors in the ARIMAX model. To do this, first I am going to use pairplots. I checked the relation of each variable with the sold count. From the visualizations, Basket Count and Visit Count and Favored Count seems to have a correlation, however from the project experiment, these 3 attributes have correlation for each other. Therefore, I choose as external regressors as one of them whivh is basket count.

#### Pair Plot

```{r,fig.width=10, warning=F}
ggpairs(pr1 ,c(4,5,6,7,8,9,10,11,12,13 ))
```
Only reasonalbe corelation looks like Basket count.

```{r,fig.width=10, warning=F}
ggpairs(pr1 ,c(4,7))
```


#### 2 ) Adding regressors to the ARIMA model

- Before creating a regressor vector, I had to split my data into train and test data so that I can calculate some performance measures. To do that, I am allocating the last seven days as test data and the rest as train data.

```{r,fig.width=10, warning=F}
traindata <- sold[-c(1:7),]
head(traindata)
testdata <- sold[c(1:7),]
head(testdata)
```

```{r,fig.width=10, warning=F}
regressor <- pr1$basket_count[-c(1:7)]
head(regressor)
```

##### Now I need to decompose my train data and fit the model with and without external regressors.

```{r,fig.width=10, warning=F}
traindatats <- ts(rev(traindata$sold_count),frequency = 7, start = c(1,1))
resultdec <- decompose(traindatats,type= "additive")
trend = resultdec$trend
season = resultdec$seasonal
random = resultdec$random 
plot(resultdec)
```

#### Without regressors

```{r,fig.width=10, warning=F}
model <- auto.arima(random)
summary(model)
```

#### With external regressor

```{r,fig.width=10, warning=F}
model <- auto.arima(random, xreg = regressor)
summary(model)
```

Added regressor has some affect on the model, we continue with this model.


#### Forecasting and model performance measures

```{r,fig.width=10, warning=F}
model <- auto.arima(random, xreg = regressor)
summary(model)
```

```{r,fig.width=10, warning=F}
model.forecast <- predict(model, n.ahead = 10, newxreg = rev(pr1$basket_count[c(1:10)]))$pred

last.trend.value <-tail(resultdec$trend[!is.na(resultdec$trend)],10)
seasonality <- resultdec$seasonal[22:31]

forecast_normalized <- model.forecast+last.trend.value+seasonality
forecast_normalized= ts(forecast_normalized, frequency = 7, start=c(5,4))
```

```{r,fig.width=10, warning=F}
testdata <- ts(rev(testdata$sold_count), frequency = 7, start=c(5,4))

plot(testdata,ylim = c(0,10))
points(forecast_normalized,type= "l", col = 2)

```

```{r,fig.width=10, warning=F}
modelfit <- random - model$residuals

plot(random)
points(modelfit, type= "l", col = 2)

```

#### Conclusion

##### Overall looking the model, arima model is not fitted well because the data set is small and we have moving average term. However, predictions looks good. I might be continue with this ARIMAX model for this model. 

## PRODUCT ID: 73318567

### TASK 1 - Analyzing Seasonality

### 1 ) Importing and Manipulating the data 

Meaningful data for this product is starting from 23rd of January 2021. Therefore, we only have 4 month data set.

```{r}
pr2 = read.csv("alldata_item2.csv")
pr2 <- as.data.table(pr2)
pr2 <- pr2[,-c("X","w_day")]
```

```{r,warning=F}
pr2 <- mutate(pr2, event_date = dmy(event_date)) # converting event date into datetime object
pr2[, Month:=as.numeric(lubridate::month(event_date,label=F))] #adding month information as a numeric variable 
pr2[, Day:=as.numeric(lubridate::wday(event_date,label=F))] #adding day information as a numeric variable 
head(pr2)
```

To built a ARIMA model, I only neeed `sold_count` output variable.

```{r}
sold <- data.table(event_date =pr2$event_date,
                   sold_count = pr2$sold_count)
head(sold)
``` 

### Visualizations

To detect outliers, first I draw boxplot

```{r, fig.width=10}
boxplot(sold$sold_count)
```

```{r,fig.width=10}
ggplot(sold, aes(x = event_date)) + 
  geom_line(aes(y = sold_count)) +  ggtitle("Product 73318567 Sold Amount") +
  xlab("Date") + ylab("Amount Sold")
```

Even if I only choose nonzero and non-NA sold counts while I am exporting data, sold_count sometimes still around 0. The reason behind it can be this product is a bikini. Therefore, in the winter times, we do nto aspect that item to sold. However, around end of February, reasonable number of item sold. This can be caused by a discount.

Now, let us check ACF and PACF plots.

#### ACF Plot

```{r,fig.width=10}
acf(sold$sold_count)
```

It seems like we have some trend in the data. We can do differization but we will use decomposition of the data. Therefore no need to do differization.

#### PACF Plot

```{r,fig.width=10}
pacf(sold$sold_count)
```

### 3 ) Decomposing the data

#### Weekly Seasonality

Actually, we only have 4 month of data points. To be able to decompose data with monthly periods, we should at least 12 month of data. Therefore, we only check weekly seasonality.

```{r,fig.width=10}
soldts <- ts(rev(pr2$sold_count),  freq = 7, start= c(1,1))
resultweekdec <- decompose(soldts,type= "additive")
plot(resultweekdec)
```

While we doing additive decomposition, random term of it has too much variance. Therefore, for this product using multiplicative decomposition is better.

```{r,fig.width=10}
soldts <- ts(rev(pr2$sold_count),  freq = 7, start= c(1,1))
resultweekdec <- decompose(soldts,type= "multiplicative")
plot(resultweekdec)
```

#### Random term after decomposing weekly

```{r,fig.width=10}
plot(resultweekdec$random)
title("Random Term of Weekly Decomposed Data")
```

Still, there is some outliers in the random term. However, we continue with this decomposed object.


```{r,fig.width=10}
random = resultweekdec$random
trend = resultweekdec$trend
season = resultweekdec$seasonal
```

### Task 2- Fitting data into ARIMA Model

#### 1 ) Deciding on the parameters

```{r,fig.width=10}
acf(random, na.action = na.pass)
```

```{r,fig.width=10}
pacf(random, na.action = na.pass)
```

From ACF and PACF plots, we can use 3 auto correlation coefficient. However, auto.arima function can also find a better model.

```{r,fig.width=10}
model <- arima(random, order= c(3,0,0))
model

```
```{r,fig.width=10}
model <- arima(random, order= c(4,0,0))
model

```
```{r,fig.width=10}
model <- arima(random, order= c(2,0,0))
model

```

```{r,fig.width=10}

model <- auto.arima(random)
model

```

From, those models we should use with 3 autoregressiove term because it has the smallest AIC value.

```{r,fig.width=10}
model <- arima(random, order= c(3,0,0))
summary(model)
```

#### Residuals after fitting

```{r,fig.width=10}
plot(model$residuals)
title("Residuals")
```

```{r,fig.width=10}
modelfit <- random - model$residuals
fitted <- modelfit*trend*season

plot(soldts)
points(fitted, type= "l", col = 2)

```

#### Comments

Even that model can not catch the jumps of the data, it has a good fit looking. Now, we can improve the model by addinbg external regressors.

### Task 3 and 4 - Adding external regressors to the model

#### 1 ) Searcing for regressors

- In this part, I am going to look for variables that might be correlated to sold amount and can be used as regressors in the ARIMAX model. To do this, first I am going to use pairplots.  I checked the relation of each variable with the sold count. From the visualizations, Basket Count and Category Favored seems to have a correlation, so I am going to add those variables to the model.

#### Pair Plot

```{r,fig.width=10, warning=F}
ggpairs(pr2, columns = c(4,7,8,10,12,13 ))
```
For this set, Basket Count and Category Basket can be used.

```{r,fig.width=10, warning=F}
ggpairs(pr2, columns = c(4,5,6,9,11))
```

From this set, we can use Visit Count attribute.

Overall, we selected our regressors as Basket Count, Category Basket and Visit Count.

#### 2 ) Adding regressors to the ARIMA model

- Before creating a regressors matrix, I had to split my data into train and test data so that I can calculate some performance measures. To do that, I am allocating the last seven days as test data and the rest as train data.

```{r,fig.width=10, warning=F}
traindata <- sold[-c(1:7),]
head(traindata)
testdata <- sold[c(1:7),]
head(testdata)
```

```{r,fig.width=10, warning=F}
regressors <- pr2[-c(1:7), c(5,7,12)]
head(regressors)
```

##### Now I need to decompose my train data and fit the model with and without external regressors.

```{r,fig.width=10, warning=F}
traindatats <- ts(rev(traindata$sold_count),frequency = 7, start = c(1,1))
resultdec <- decompose(traindatats,type= "multiplicative")
trend = resultdec$trend
season = resultdec$seasonal
random = resultdec$random 
plot(resultdec)
```

#### Without regressors

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(3,0,0))
summary(model)
```

#### With external regressor

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(3,0,0), xreg = as.matrix(regressors))
summary(model)
```
```{r,fig.width=10, warning=F}
model <- auto.arima(random, xreg = as.matrix(regressors))
summary(model)
```

When we add 3 regressors, none of them has the significancy in the model, for the last time I am going to try to build arima model with only one external regressor.


```{r,fig.width=10, warning=F}
regressors <- pr2[-c(1:7), 7]
head(regressors)
```

```{r,fig.width=10, warning=F}
model <- auto.arima(random, xreg = as.matrix(regressors))
summary(model)
```

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(3,0,0), xreg = regressors)
summary(model)
```

External regressor does not affect the model, but we can hold it into the model and forecast with it.

#### Forecasting and model performance measures

```{r,fig.width=10, warning=F}
model.forecast <- predict(model, n.ahead= 10, newxreg = pr2$basket_count[c(1:10)] )$pred

last.trend.value <- tail(resultdec$trend[!is.na(resultdec$trend)],1)
seasonality <- resultdec$seasonal[4:13]

forecast_normalized <- model.forecast*last.trend.value*seasonality
forecast_normalized= ts(forecast_normalized, frequency = 7, start=c(15,4))
```

```{r,fig.width=10, warning=F}
testdata <- ts(rev(testdata$sold_count), frequency = 7, start=c(15,4))

plot(testdata, ylim = c(30,80))
points(forecast_normalized,type= "l", col = 2)

```

```{r,fig.width=10, warning=F}
modelfit <- random - model$residuals

plot(random)
points(modelfit, type= "l", col = 2)

```

#### Conclusion

##### The prediction is around the actual values, but still cannot detect outliers. When we add regressors also cannot affect the model, so we predict only with 3 autoregressive term for this product. In LM model, we are using regressors and it predict better than this. Therefore, we continue with LM model. 


## PRODUCT ID: 32737302

### TASK 1 - Analyzing Seasonality

### 1 ) Importing and Manipulating the data 

Meaningful data for this product is starting from 20th of February 2021. Therefore, we only have 119 data row.

```{r}
pr3 = read.csv("alldata_item3.csv")
pr3 <- as.data.table(pr3)
pr3 <- pr3[,-c("X","w_day")]
pr3 <- pr3[c(1:119),]
```

```{r,warning=F}
pr3 <- mutate(pr3, event_date = dmy(event_date)) # converting event date into datetime object
pr3[, Month:=as.numeric(lubridate::month(event_date,label=F))] #adding month information as a numeric variable 
pr3[, Day:=as.numeric(lubridate::wday(event_date,label=F))] #adding day information as a numeric variable 
head(pr3)
```

To built a ARIMA model, I only need `sold_count` output variable.

```{r}
sold <- data.table(event_date =pr3$event_date,
                   sold_count = pr3$sold_count)
head(sold)
``` 

### Visualizations

To detect outliers, first I draw boxplot:

```{r, fig.width=10}
boxplot(sold$sold_count)
```

```{r,fig.width=10}
ggplot(sold, aes(x = event_date)) + 
  geom_line(aes(y = sold_count)) +  ggtitle("Product 32737302 Sold Amount") +
  xlab("Date") + ylab("Amount Sold")
```

This product is also a bikini. Thereefore, we aspect naturally that sold count should have an icreased trend while we are getting closer to the summer days. However at March also the sales are high, it can be because of some discount

Now, let us check ACF and PACF plots.

#### ACF Plot

```{r,fig.width=10}
acf(sold$sold_count)
```

From ACF plot, there can be a weekly seasonality in the data.

#### PACF Plot

```{r,fig.width=10}
pacf(sold$sold_count)
```
PACF plot had a peak at lag 7. Maybe we can detect seasonality when we do weekly decomposition.

### 3 ) Decomposing the data

#### Weekly Seasonality

Actually, we only have 119 row of data. To be able to decompose data with monthly periods, we should at least 12 month of data. Therefore, we only check weekly seasonality.

```{r,fig.width=10}
soldts <- ts(rev(pr3$sold_count),  freq = 7, start= c(1,1))
resultweekdec <- decompose(soldts,type= "additive")
plot(resultweekdec)
```

#### Random term after decomposing weekly

```{r,fig.width=10}
plot(resultweekdec$random)
title("Random Term of Weekly Decomposed Data")
```

Random term looks like, zero mean and constant variance. Therefore we continue with this random term to built our ARIMA and ARIMAX models.

```{r,fig.width=10}
random = resultweekdec$random
trend = resultweekdec$trend
season = resultweekdec$seasonal
```

### Task 2- Fitting data into ARIMA Model

#### 1 ) Deciding on the parameters

```{r,fig.width=10}
acf(random, na.action = na.pass)
```
We can use 4 autoregressive model but we also need to use auto.arima function.

```{r,fig.width=10}
pacf(random, na.action = na.pass)
```

ACF and PACF models shows us that this random term need moving average terms.

```{r,fig.width=10}
model <- auto.arima(random)
model

```

```{r,fig.width=10}
model <- arima(random, order= c(0,0,2))
summary(model)
```


Better to go with two moving average term respect to the AIC values.

#### Residuals after fitting

```{r,fig.width=10}
plot(model$residuals)
title("Residuals")
```
```{r,fig.width=10}
modelfit <- random - model$residuals
fitted <- modelfit+trend+season

plot(soldts)
points(fitted, type= "l", col = 2)

```
#### Comments

It has a good fit looking. Now, we can improve the model by adding external regressors.

### Task 3 and 4 - Adding external regressors to the model

#### 1 ) Searcing for regressors

- In this part, I am going to look for variables that might be correlated to sold amount and can be used as regressors in the ARIMAX model. To do this, first I am going to use pairplots.  I checked the relation of each variable with the sold count.

#### Pair Plot

```{r,fig.width=10, warning=F}
ggpairs(pr3, columns = c(4,7,8,10,12,13 ))
```
For this set, Basket Count and Category Sold can be used.

```{r,fig.width=10, warning=F}
ggpairs(pr3, columns = c(4,5,6,9,11))
```

For this set, we can use Visit Count.

Overall, we selected our regressors as Basket Count, Category Sold and Visit Count.

#### 2 ) Adding regressors to the ARIMA model

- Before creating a regressors matrix, I had to split my data into train and test data so that I can calculate some performance measures. To do that, I am allocating the last seven days as test data and the rest as train data.

```{r,fig.width=10, warning=F}
traindata <- sold[-c(1:7),]
head(traindata)
testdata <- sold[c(1:7),]
head(testdata)
```

```{r,fig.width=10, warning=F}
regressors <- pr3[-c(1:7), c(5,7,8)]
head(regressors)
```

##### Now I need to decompose my train data and fit the model with and without external regressors.

```{r,fig.width=10, warning=F}
traindatats <- ts(rev(traindata$sold_count),frequency = 7, start = c(1,1))
resultdec <- decompose(traindatats,type= "additive")
trend = resultdec$trend
season = resultdec$seasonal
random = resultdec$random 
plot(resultdec)
```

#### Without regressors

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(0,0,2))
summary(model)
```

#### With external regressor

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(0,0,2), xreg = regressors)
summary(model)
```

Coefficients of earlier model has been changed, category sold attribute useless for the model. That's probably because some regressors explains each other. I am going to drop the Category Sold column to try to fit the model again.

```{r,fig.width=10, warning=F}
regressors <- pr3[-c(1:7), c(5,7)]
head(regressors)
```

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(0,0,2), xreg = regressors)
summary(model)
```

Now, we have better moving average and external regressor terms. We  can continue with forecasting.

#### Forecasting and model performance measures

```{r,fig.width=10, warning=F}

regmatrix <- pr3[c(1:10), c(5,7)]
model.forecast <- predict(model, n.ahead= 10, newxreg = regmatrix )$pred

last.trend.value <- tail(resultdec$trend[!is.na(resultdec$trend)],10)
seasonality <- resultdec$seasonal[96:105]

forecast_normalized <- model.forecast+last.trend.value+seasonality
forecast_normalized= ts(forecast_normalized, frequency = 7, start=c(16,4))
```

```{r,fig.width=10, warning=F}
testdata <- ts(rev(testdata$sold_count), frequency = 7, start=c(16,4))

plot(testdata)
points(forecast_normalized,type= "l", col = 2)

```
```{r,30,fig.width=10, warning=F}
modelfit <- random - model$residuals

plot(random)
points(modelfit, type= "l", col = 2)
title("Fitted vs Actual")

```
#### Conclusion

##### The predictions are around average sales, however it can not detect very well peaks as we can see both predictions and fitted plot. However, we can use this model for predictions but at some points, peaks should be excluded.


## Product ID: 31515569

```{r, warning=FALSE}
pr4 <- read_excel("item4.xlsx")
ggplot(data = pr4, aes(x = event_date, y = sold_count))+ geom_point()+ geom_line(aes(group=1))
testpr4 <- tail(pr4, 7)
trainpr4 <- head(pr4, -(7))
```

Looking at the daily sales graph, it can be initially stated that the data set does not have any obvious trend. To comment on seasonality, further investigation is needed since the data points are mostly congested around similar values.

### Conversion into Time Series and Decomposition

The daily sales data for product 31515569 will be converted into weekly and monthly time series data. Then, each time series data will be decomposed to analyze the seasonality and trend properties. 

#### Weekly Time Series

```{r, warning=FALSE}
weeklytspr4 <- ts(trainpr4$sold_count, freq = 7, start = c(1,1))
decweeklypr4 <- decompose(weeklytspr4, type = "additive")
plot(decweeklypr4)
tsdisplay(decweeklypr4$seasonal)
tsdisplay(decweeklypr4$random)
kpss.test(decweeklypr4$random)
```

The time series data for weekly level is constructed above. Since variance of the initial sales data almost does not change except for some sudden peaks, additive decomposition is applied. Looking at the plot of decomposed data, trend component does not only increase or decrease, hence no trend is observed. When the seasonal component is investigated, it is seen that the seasonal component repeats a certain path. Also, in the ACF plot of seasonal component, there is high autocorrelation between lags that are multiples of 7, hence there is weekly seasonality. Besides, random component turns out to be stationary when KPSS test is applied.

#### Monthly Time Series

```{r, warning=FALSE}
monthlytspr4 <- ts(trainpr4$sold_count, freq = 30, start = c(1,1))
decmonthlypr4 <- decompose(monthlytspr4, type = "additive")
plot(decmonthlypr4)
tsdisplay(decmonthlypr4$seasonal)
tsdisplay(decmonthlypr4$random)
kpss.test(decmonthlypr4$random)
```

The time series data for monthly level is constructed above. Since variance of the initial sales data almost does not change except for some sudden peaks, additive decomposition is applied. Looking at the plot of decomposed data, trend component does not only increase or decrease, hence no trend is observed. When the seasonal component is investigated, it is seen that the seasonal component repeats a certain path. Also, in the ACF plot of seasonal component, there is high autocorrelation between lags that are multiples of 30, hence there is monthly seasonality. Besides, random component turns out to be stationary when KPSS test is applied.

### ARIMA Models

#### Selection of the Model

The random component of weekly time series decomposition will be used to construct an ARIMA model as it looks more random than the other candidate. Looking at the ACF and PACF plots of the random term, ARIMA(2,0,2) is chosen as the initial model. Then, selected model will be improved by doing neighborhood search.

```{r, warning=FALSE}
randompr4 <- decweeklypr4$random

pr4arima <- arima(randompr4, order=c(2,0,2))
pr4arima

pr4ar1 <- arima(randompr4, order=c(1,0,2))
pr4ar1 

pr4ar2 <- arima(randompr4, order=c(3,0,2))
pr4ar2 #lowest AIC

pr4ar3 <- arima(randompr4, order=c(2,0,1))
pr4ar3 

pr4ar4 <- arima(randompr4, order=c(2,0,3))
pr4ar4 

pr4model <- pr4ar2
```

After neighborhood search, the previously selected model is improved and the final model turns out to be ARIMA(3,0,2).

#### Model Analysis

The plot of fitted model vs real values is shown below.

```{r, warning=FALSE}
model_fitted_pr4 <- randompr4 - residuals(pr4model)
model_fitted_transformed_pr4 <- model_fitted_pr4+decweeklypr4$trend+decweeklypr4$seasonal
plot(weeklytspr4, xlab = "Weeks", ylab = "Sold Count",main="Actual (Black) vs. Predicted (Blue)")+points(model_fitted_transformed_pr4, type = "l", col = 5, lty = 1)
```

To have an idea about the performance of the model, residuals of the model will be investigated.

```{r, warning=FALSE}
checkresiduals(pr4model,lag=70)
```

Looking at the residuals plot, it can be stated that zero mean and constant variance assumptions do not exactly hold. Besides, the ACF plot shows that residuals are not exactly random. As a result, the model is not adequate and can be improved by adding external regressors.

### Checking for Regressors

In this part, relation of sold count data with other regressors will be tested to see whether the model can be improved by using external regressors. Because of the dirty nature of the data, only "basket count", "category sold", "category visits", and "category favored" columns will be tested using pairplots.

```{r, warning=FALSE}
pr4data <- data.frame(date = trainpr4$event_date, sold_count = trainpr4$sold_count, basket_count = trainpr4$basket_count, category_visits = trainpr4$category_visits, category_favored = trainpr4$category_favored)
pairs(pr4data)
```

According to the pairplot, "basket count" and "category favored" columns are chosen to be added to the model as external regressors.

### Adding Regressors to the Model

As chosen in the previous section, "basket count" and "category favored" data will be added to the model as external regressors.

```{r, warning=FALSE}
regressorspr4 <- data.frame(trainpr4$basket_count, trainpr4$category_favored)
arimaxpr4 <- arima(randompr4, order = c(3,0,2), xreg = regressorspr4)
arimaxpr4
```

Looking at the significance values of the regressors, it can be said that the second regressor "category favored" has lost its significance. Therefore, the model can be built using the first regressor only.

```{r, warning=FALSE}
arimaxpr4 <- arima(randompr4, order = c(3,0,2), xreg = pr4data$basket_count)
arimaxpr4
```

After the addition of regressors, as a first impression, it can be observed that the AIC value falls, hence the model improves. To make a further investigation, fitted model vs real values graph can be plotted and residuals can be analyzed.

```{r, warning=FALSE}
xregmodel_fitted_pr4 <- randompr4 - residuals(arimaxpr4)
xregmodel_fitted_transformed_pr4 <- xregmodel_fitted_pr4+decweeklypr4$trend+decweeklypr4$seasonal
plot(weeklytspr4, xlab = "Weeks", ylab = "Sold Count",main="Actual (Black) vs. Predicted (Blue)")+points(xregmodel_fitted_transformed_pr4, type = "l", col = 5, lty = 1)
checkresiduals(arimaxpr4,lag=70)
```

Compared to the ARIMA(3,0,2) model, which does not contain any external regressor, the model has just slightly improved with this ARIMAX model.

### Forecasting and Performance Measures

Predictions for the next 7 days are shown below.

```{r, warning=FALSE}
model_forecast_pr4 = predict(arimaxpr4, n.ahead=7, newxreg = pr4data$basket_count[383:389])$pred
last_trend_pr4 = tail(decweeklypr4$trend[!is.na(decmonthlypr4$trend)], 1)
seasonality_pr4 = decweeklypr4$seasonal[5:11]
model_forecast_pr4 = model_forecast_pr4+last_trend_pr4+seasonality_pr4
forecastpr4 <- data.frame(date = testpr4$event_date, actual = testpr4$sold_count, predicted = model_forecast_pr4)
forecastpr4
```

To observe the performance of the model, actual test data vs predicted values graph will be plotted.

```{r, warning=FALSE}
ggplot() + 
  geom_line(data = forecastpr4, aes(x = date, y = predicted,color = "predicted")) +
  geom_line(data = forecastpr4, aes(x = date, y = actual,color = "actual")) +
  xlab('Date') +
  ylab('Sold Count')
```

### Conclusion

As seen above, predictions are not so far from the actual data. However, it can be said that the last form of the model is not the best that can be built. To improve the model, outlier data points may be ignored and the model can be constructed again. Alternatively, LM models can be tried.
               
## Product ID: 6676673

```{r, warning=FALSE}
pr5 <- read_excel("item5.xlsx")
ggplot(data = pr5, aes(x = event_date, y = sold_count))+ geom_point()+ geom_line(aes(group=1))
testpr5 <- tail(pr5, 7)
trainpr5 <- head(pr5, -(7))
```

Looking at the daily sales graph, it can be initially stated that the data set does not have any obvious trend. To comment on seasonality, further investigation is needed.

### Conversion into Time Series and Decomposition

The daily sales data for product 6676673 will be converted into weekly and monthly time series data. Then, each time series data will be decomposed to analyze the seasonality and trend properties. 

#### Weekly Time Series

```{r, warning=FALSE}
weeklytspr5 <- ts(trainpr5$sold_count, freq = 7, start = c(1,1))
decweeklypr5 <- decompose(weeklytspr5, type = "additive")
plot(decweeklypr5)
tsdisplay(decweeklypr5$seasonal)
tsdisplay(decweeklypr5$random)
kpss.test(decweeklypr5$random)
```

The time series data for weekly level is constructed above. Since variance does not change significantly over time, additive decomposition is applied. Looking at the plot of decomposed data, trend component does not only increase or decrease, hence no trend is observed. When the seasonal component is investigated, it is seen that the seasonal component repeats a certain path. Also, in the ACF plot of seasonal component, there is high autocorrelation between lags that are multiples of 7, hence there is weekly seasonality. Besides, random component turns out to be stationary when KPSS test is applied.

#### Monthly Time Series

```{r, warning=FALSE}
monthlytspr5 <- ts(trainpr5$sold_count, freq = 30, start = c(1,1))
decmonthlypr5 <- decompose(monthlytspr5, type = "additive")
plot(decmonthlypr5)
tsdisplay(decmonthlypr5$seasonal)
tsdisplay(decmonthlypr5$random)
kpss.test(decmonthlypr5$random)
```

The time series data for monthly level is constructed above. Since variance variance does not change significantly over time, additive decomposition is applied. Looking at the plot of decomposed data, trend component does not only increase or decrease, hence no trend is observed. When the seasonal component is investigated, it is seen that the seasonal component repeats a certain path. Also, in the ACF plot of seasonal component, there is high autocorrelation between lags that are multiples of 30, hence there is monthly seasonality. Besides, random component turns out to be stationary when KPSS test is applied.

#### Three-Daily Time Series

While investigating the weekly time series, it is realized that there is high autocorrelation between random terms in every three days. Hence, it is decided to build a three-daily time series data and use its random term while constructing an ARIMA model.

```{r, warning=FALSE}
threetspr5 <- ts(trainpr5$sold_count, freq = 3, start = c(1,1))
decthreepr5 <- decompose(threetspr5, type = "additive")
plot(decthreepr5)
tsdisplay(decthreepr5$seasonal)
tsdisplay(decthreepr5$random)
kpss.test(decthreepr5$random)
```

### ARIMA Models

#### Selection of the Model

After the investigation of time series decompositions at different levels, random term of three-daily time series decomposition is chosen to build the model as it is more random than the other candidates. Looking at the ACF and PACF plots of the random term, ARIMA(0,0,2) is chosen as the initial model. Then, selected model will be improved by doing neighborhood search.

```{r, warning=FALSE}
randompr5 <- decthreepr5$random

pr5arima <- arima(randompr5, order = c(0,0,2)) 
pr5arima

pr5ar1 <- arima(randompr5, order=c(0,0,1))
pr5ar1

pr5ar2 <- arima(randompr5, order=c(0,0,3))
pr5ar2

pr5ar3 <- arima(randompr5, order=c(1,0,2))
pr5ar3 #lowest AIC

pr5model <- pr5ar3
```

After neighborhood search, the previously selected model is improved and the final model turns out to be ARIMA(1,0,2).

#### Model Analysis

The plot of fitted model vs real values is shown below.

```{r, warning=FALSE}
model_fitted_pr5 <- randompr5 - residuals(pr5model)
model_fitted_transformed_pr5 <- model_fitted_pr5+decthreepr5$trend+decthreepr5$seasonal
plot(threetspr5, xlab = "3-Days", ylab = "Sold Count",main="Actual (Black) vs. Predicted (Blue)")+points(model_fitted_transformed_pr5, type = "l", col = 5, lty = 1)
```

To have an idea about the performance of the model, residuals of the model will be investigated.

```{r, warning=FALSE}
checkresiduals(pr5model,lag=70)
```

Looking at the residuals plot, it can be stated that zero mean assumption does not exactly hold. To improve the current model, external regressors can be added and an ARIMAX model can be built.

### Checking for Regressors

In this part, relation of sold count data with other regressors will be tested to see whether the model can be improved by using external regressors. Because of the dirty nature of the data, only "basket count", "category sold", "category visits", and "category favored" columns will be tested using pairplots.

```{r, warning=FALSE}
pr5data <- data.frame(date = trainpr5$event_date, sold_count = trainpr5$sold_count, basket_count = trainpr5$basket_count, category_visits = trainpr5$category_visits, category_favored = trainpr5$category_favored)
pairs(pr5data)
```

According to the pairplot, "basket count" and "category favored" columns are chosen to be added to the model as external regressors.

### Adding Regressors to the Model

As chosen in the previous section, "basket count" and "category favored" data will be added to the model as external regressors.

```{r, warning=FALSE}
regressorspr5 <- data.frame(trainpr5$basket_count, trainpr5$category_favored)
arimaxpr5 <- arima(randompr5, order = c(1,0,2), xreg = regressorspr5)
arimaxpr5
```

Looking at the significance values of the regressors, it can be said that the second regressor "category favored" has lost its significance. Therefore, the model can be built using the first regressor only.

```{r, warning=FALSE}
arimaxpr5 <- arima(randompr5, order = c(1,0,2), xreg = pr5data$basket_count)
arimaxpr5
```

After the addition of regressors, as a first impression, it can be observed that the AIC value falls, hence the model improves. To make a further investigation, fitted model vs real values graph can be plotted and residuals can be analyzed.

```{r, warning=FALSE}
xregmodel_fitted_pr5 <- randompr5 - residuals(arimaxpr5)
xregmodel_fitted_transformed_pr5 <- xregmodel_fitted_pr5+decthreepr5$trend+decthreepr5$seasonal
plot(threetspr5, xlab = "3-Days", ylab = "Sold Count",main="Actual (Black) vs. Predicted (Blue)")+points(xregmodel_fitted_transformed_pr5, type = "l", col = 5, lty = 1)
checkresiduals(arimaxpr5,lag=70)
```

Compared to the ARIMA(1,0,2) model, which does not contain any external regressor, the model has just slightly improved in this ARIMAX model. 

### Forecasting and Performance Measures

Predictions for the next 7 days are shown below.

```{r, warning=FALSE}
model_forecast_pr5 = predict(arimaxpr5, n.ahead=7, newxreg = pr5data$basket_count[383:389])$pred
last_trend_pr5 = tail(decthreepr5$trend[!is.na(decmonthlypr5$trend)], 1)
seasonality_pr5 = decthreepr5$seasonal[3:9]
model_forecast_pr5 = model_forecast_pr5+last_trend_pr5+seasonality_pr5
forecastpr5 <- data.frame(date = testpr5$event_date, actual = testpr5$sold_count, predicted = model_forecast_pr5)
forecastpr5
```

To observe the performance of the model, actual test data vs predicted values graph will be plotted.

```{r, warning=FALSE}
ggplot() + 
  geom_line(data = forecastpr5, aes(x = date, y = predicted,color = "predicted")) +
  geom_line(data = forecastpr5, aes(x = date, y = actual,color = "actual")) +
  xlab('Date') +
  ylab('Sold Count')
```

### Conclusion

As seen above, predictions are not so far from the actual data. However, it can be said that the last form of the model is not the best that can be built. To improve the model, outlier data points may be ignored and the model can be constructed again. Alternatively, LM models can be tried.

## Product ID: 7061886

```{r, warning=FALSE}
pr6 <- read_excel("item6.xlsx")
ggplot(data = pr6, aes(x = event_date, y = sold_count))+ geom_point()+ geom_line(aes(group=1))
testpr6 <- tail(pr6, 7)
trainpr6 <- head(pr6, -(7))
```

Looking at the daily sales graph, it can be initially stated that the data set does not have any obvious trend. To comment on seasonality, further investigation is needed since the data points are mostly congested around similar values.

### Conversion into Time Series and Decomposition

The daily sales data for product 7061886 will be converted into weekly and monthly time series data. Then, each time series data will be decomposed to analyze the seasonality and trend properties. 

#### Weekly Time Series

```{r, warning=FALSE}
weeklytspr6 <- ts(trainpr6$sold_count, freq = 7, start = c(1,1))
decweeklypr6 <- decompose(weeklytspr6, type = "additive")
plot(decweeklypr6)
tsdisplay(decweeklypr6$seasonal)
tsdisplay(decweeklypr6$random)
kpss.test(decweeklypr6$random)
```

The time series data for weekly level is constructed above. Since variance does not change significantly over time, additive decomposition is applied. Looking at the plot of decomposed data, trend component does not only increase or decrease, hence no trend is observed. When the seasonal component is investigated, it is seen that the seasonal component repeats a certain path. Also, in the ACF plot of seasonal component, there is high autocorrelation between lags that are multiples of 7, hence there is weekly seasonality. Besides, random component turns out to be stationary when KPSS test is applied.

#### Monthly Time Series

```{r, warning=FALSE}
monthlytspr6 <- ts(trainpr6$sold_count, freq = 30, start = c(1,1))
decmonthlypr6 <- decompose(monthlytspr6, type = "additive")
plot(decmonthlypr6)
tsdisplay(decmonthlypr6$seasonal)
tsdisplay(decmonthlypr6$random)
kpss.test(decmonthlypr6$random)
```

The time series data for monthly level is constructed above. Since variance variance does not change significantly over time, additive decomposition is applied. Looking at the plot of decomposed data, trend component does not only increase or decrease, hence no trend is observed. When the seasonal component is investigated, it is seen that the seasonal component repeats a certain path. Also, in the ACF plot of seasonal component, there is high autocorrelation between lags that are multiples of 30, hence there is monthly seasonality. Besides, random component turns out to be stationary when KPSS test is applied.

### ARIMA Models

#### Selection of the Model

The random component of weekly time series decomposition will be used to construct an ARIMA model as it looks more random than the other candidate. Looking at the ACF and PACF plots of the random term, ARIMA(1,0,2) is chosen as the initial model. Then, selected model will be improved by doing neighborhood search.

```{r, warning=FALSE}
randompr6 <- decweeklypr6$random

pr6arima <- arima(randompr6, order=c(1,0,2))
pr6arima

pr6ar1 <- arima(randompr6, order=c(0,0,2))
pr6ar1 

pr6ar2 <- arima(randompr6, order=c(2,0,2))
pr6ar2 

pr6ar3 <- arima(randompr6, order=c(1,0,1))
pr6ar3 

pr6ar4 <- arima(randompr6, order=c(1,0,3))
pr6ar4 #lowest AIC

pr6model <- pr6ar4
```

After neighborhood search, the previously selected model is improved and the final model turns out to be ARIMA(1,0,3).

#### Model Analysis

The plot of fitted model vs real values is shown below.

```{r, warning=FALSE}
model_fitted_pr6 <- randompr6 - residuals(pr6model)
model_fitted_transformed_pr6 <- model_fitted_pr6+decweeklypr6$trend+decweeklypr6$seasonal
plot(weeklytspr6, xlab = "Weeks", ylab = "Sold Count",main="Actual (Black) vs. Predicted (Blue)")+points(model_fitted_transformed_pr6, type = "l", col = 5, lty = 1)
```

To have an idea about the performance of the model, residuals of the model will be investigated.

```{r, warning=FALSE}
checkresiduals(pr6model,lag=70)
```

Looking at the residuals plot, it can be stated that constant variance assumption does not exactly hold. Besides, the ACF plot shows that residuals are a little bit correlated. To improve the current model, external regressors can be added and an ARIMAX model can be built.

### Checking for Regressors

In this part, relation of sold count data with other regressors will be tested to see whether the model can be improved by using external regressors. Because of the dirty nature of the data, only "basket count", "category sold", "category visits", and "category favored" columns will be tested using pairplots.

```{r, warning=FALSE}
pr6data <- data.frame(date = trainpr6$event_date, sold_count = trainpr6$sold_count, basket_count = trainpr6$basket_count, category_visits = trainpr6$category_visits, category_favored = trainpr6$category_favored)
pairs(pr6data)
```

According to the pairplot, "basket count" and "category favored" columns are chosen to be added to the model as external regressors.

### Adding Regressors to the Model

As chosen in the previous section, "basket count" and "category favored" data will be added to the model as external regressors.

```{r, warning=FALSE}
regressorspr6 <- data.frame(trainpr6$basket_count, trainpr6$category_favored)
arimaxpr6 <- arima(randompr6, order = c(1,0,3), xreg = regressorspr6)
arimaxpr6
```

Looking at the significance values of the regressors, it can be said that the second regressor "category favored" has lost its significance. Therefore, the model can be built using the first regressor only.

```{r, warning=FALSE}
arimaxpr6 <- arima(randompr6, order = c(1,0,3), xreg = pr6data$basket_count)
arimaxpr6
```

After the addition of regressors, as a first impression, it can be observed that the AIC value slightly falls, hence the model just improves. To make a further investigation, fitted model vs real values graph can be plotted and residuals can be analyzed.

```{r, warning=FALSE}
xregmodel_fitted_pr6 <- randompr6 - residuals(arimaxpr6)
xregmodel_fitted_transformed_pr6 <- xregmodel_fitted_pr6+decweeklypr6$trend+decweeklypr6$seasonal
plot(weeklytspr6, xlab = "Weeks", ylab = "Sold Count",main="Actual (Black) vs. Predicted (Blue)")+points(xregmodel_fitted_transformed_pr6, type = "l", col = 5, lty = 1)
checkresiduals(arimaxpr6,lag=70)
```

Compared to the ARIMA(1,0,3) model, which does not contain any external regressor, autocorrelation of the residuals does not change much in this ARIMAX model. 

### Forecasting and Performance Measures

Predictions for the next 7 days are shown below.

```{r, warning=FALSE}
model_forecast_pr6 = predict(arimaxpr6, n.ahead=7, newxreg = pr6data$basket_count[383:389])$pred
last_trend_pr6 = tail(decweeklypr6$trend[!is.na(decweeklypr6$trend)], 1)
seasonality_pr6 = decweeklypr6$seasonal[5:11]
model_forecast_pr6 = model_forecast_pr6+last_trend_pr6+seasonality_pr6
forecastpr6 <- data.frame(date = testpr6$event_date, actual = testpr6$sold_count, predicted = model_forecast_pr6)
forecastpr6
```

To observe the performance of the model, actual test data vs predicted values graph will be plotted.

```{r, warning=FALSE}
ggplot() + 
  geom_line(data = forecastpr6, aes(x = date, y = predicted,color = "predicted")) +
  geom_line(data = forecastpr6, aes(x = date, y = actual,color = "actual")) +
  xlab('Date') +
  ylab('Sold Count')
```

### Conclusion

As seen above, predictions are a bit off from the actual data. It can be said that the last form of the model is not the best that can be built. To improve the model, outlier data points may be ignored and the model can be constructed again. Alternatively, LM models can be tried.



## PRODUCT ID: 85004

### TASK 1 - Analyzing Seasonality

### 1 ) Importing and Manipulating the data

- After importing the recent data and deleting unnecessary columns, I changed the event_date column into datetime object to extract month,day,week information quickly. Then, I set the event_date column to index to make it time series object. Lastly, I added the month, week and day information as a numeric variable for possibility of using in future model.

```{r}
pr7 = read.csv("alldata_item7.csv")
pr7 <- as.data.table(pr7)
pr7 <- pr7[,-c("X","w_day")]
```

```{r,warning=F}
pr7 <- mutate(pr7, event_date = ymd(event_date)) # converting event date into datetime object
pr7[, Month:=as.numeric(lubridate::month(event_date,label=F))] #adding month information as a numeric variable 
pr7[, Day:=as.numeric(lubridate::wday(event_date,label=F))] #adding day information as a numeric variable 
head(pr7)
```
- For time series analysis, I only deal with the sold amount of the product so I drop possible regressors

```{r}
sold <- data.table(event_date =pr7$event_date,
                   sold_count = pr7$sold_count)
head(sold)
```

### Visualizations

- At first, since there are some dramatic peaks at some point, I wanted to check whether there is outlier points in the data or not. So I plotted the boxplot of the sold amount data. From the plot, it seems that there are some outlier points. If the model I am going to fit is not explaining those points. I might discard them.

```{r, fig.width=10}
boxplot(sold$sold_count)
```
- For initial observation, I plotted the sold_count data over time. At first sight, seasonality component seems to be exist but further examination was needed. So, I used ACF and PACF plots. From ACF, we see that there is a seasonality pattern in the data.

```{r,fig.width=10}
ggplot(sold, aes(x = event_date)) + 
  geom_line(aes(y = sold_count)) +  ggtitle("Product 85004 Sold Amount") +
  xlab("Date") + ylab("Amount Sold")

```

#### ACF Plot

```{r,fig.width=10}
acf(sold$sold_count)
```

#### PACF Plot

```{r,fig.width=10}
pacf(sold$sold_count)
```

### 3 ) Decomposing the data

#### Weekly Seasonality

- From the seasonal component, we see that there is pattern occurs every week in the sold amount data. For the trend part, it is hard to say there is a significant trend component. After decomposing the series, random term seemed to have the zero mean assumption which is useful in fitting.

#### Weekly Decomposition

```{r,fig.width=10}
soldts <- ts(rev(pr7$sold_count),  freq = 7, start= c(1,1))
resultweekdec <- decompose(soldts,type= "multiplicative")
plot(resultweekdec)
```

#### Monthly  Seasonality

- Wee again see a seasonal component in the monthly decomposition however again it is hard to say that there is clear trend pattern.

#### Monthly  Decomposition

```{r,fig.width=10}

soldtsmonth <- ts(rev(pr7$sold_count),  freq = 30, start= c(1,1))
resultmonthdec <- decompose(soldtsmonth,type= "multiplicative")
plot(resultmonthdec)
```

#### Random term after decomposing weekly

```{r,fig.width=10}
plot(resultweekdec$random)
title("Random Term of Weekly Decomposed Data")
```

#### Random term after decomposing monthly

```{r,fig.width=10}
plot(resultmonthdec$random)
title("Random Term of Monthly  Decomposed Data")
```

#### Conclusion

- Both the monthly and the weekly decompositon seems to have a significant seasonal component but the trend variables are not significant. We might use weekly decompositon as the random term of weekly decomposition seems more like white noise series.

```{r,fig.width=10}
random = resultweekdec$random
trend = resultweekdec$trend
season = resultweekdec$seasonal
```

### Task 2- Fitting data into ARIMA Model

#### 1 ) Deciding on the parameters

- To decide on the parameters of the ARIMA model, I checked the ACF and PACF plots of the random term. ACF plot has a sinusodial pattern and PACF function has a negative and big correlation at lag 3.I am going to check the neighborhood to find the best model.

```{r,fig.width=10}
acf(random, na.action = na.pass)
```

```{r,fig.width=10}
pacf(random, na.action = na.pass)
```

```{r,fig.width=10}
model <- arima(random, order= c(1,0,0))
AIC(model)

model <- arima(random, order= c(2,0,0))
AIC(model)

model <- arima(random, order= c(3,0,0))
AIC(model)

model <- arima(random, order= c(4,0,0))
AIC(model)

model <- arima(random, order= c(5,0,0))
AIC(model)

model <- arima(random, order= c(6,0,0))
AIC(model)

model <- arima(random, order= c(1,0,1))
AIC(model)

model <- arima(random, order= c(2,0,1))
AIC(model)

model <- arima(random, order= c(3,0,1))
AIC(model)

model <- arima(random, order= c(4,0,1))
AIC(model)

model <- arima(random, order= c(2,0,2))
AIC(model)

model <- arima(random, order= c(1,0,3))
AIC(model)

model <- arima(random, order= c(2,0,4))
AIC(model)

```
- So the best model we came up with is the one with (2,0,4) three autoregressive term and one moving average term.

#### 2 ) Fitting the Model

```{r,fig.width=10}
model <- arima(random, order= c(2,0,4))
summary(model)
```

#### Residuals after fitting

```{r,fig.width=10}
plot(model$residuals)
title("Residuals")
```

```{r,fig.width=10}
modelfit <- random - model$residuals
fitted <- modelfit*trend*season

plot(soldts)
points(fitted, type= "l", col = 2)

```

#### Comments

- The model seems to have a good fit since zero mean assumption and constant varinace assumption seems to be held but it can be improved adding extra regressors to the model.

### Task 3 and 4 - Adding external regressors to the model

#### 1 ) Searcing for regressors

- In this part, I am going to look for variables that might be correlated to sold amount and can be used as regressors in the ARIMAX model. To do this, first I am going to use pairplots. But some of the data in the columns are lost for so many data points. So only columns I can use for this, "Basket Count" , "Category Sold" , Category Visits" and "Category Favored". I checked the relation of each variable with the sold count. From the visualizations, Basket Count and Category Favored seems to have a correlation, so I am going to add those variables to the model.

#### Pair Plot

```{r,fig.width=10, warning=F}
ggpairs(pr7, columns = c(4,7,8,10,12 ))
```


#### 2 ) Adding regressors to the ARIMA model

- Before creating a regressors matrix, I had to split my data into train and test data so that I can calculate some performance measures. To do that, I am allocating the last seven days as test data and the rest as train data.

```{r,fig.width=10, warning=F}
traindata <- sold[-c(1:7),]
head(traindata)
testdata <- sold[c(1:7),]
head(testdata)
```

```{r,fig.width=10, warning=F}
regressors <- pr7[-c(1:7), c(7,13)]
head(regressors)
```

##### Now I need to decompose my train data and fit the model with and without external regressors.

```{r,fig.width=10, warning=F}
traindatats <- ts(rev(traindata$sold_count),frequency = 7, start = c(1,1))
resultdec <- decompose(traindatats,type= "multiplicative")
trend = resultdec$trend
season = resultdec$seasonal
random = resultdec$random 
plot(resultdec)
```

#### Without regressors

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(2,0,4))
summary(model)
```

#### With external regressor

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(2,0,4), xreg = regressors)
summary(model)
```

##### Coefficients of earlier model has been changed, some variables became insignificant. That's probably because some regressors explains each other. I am going to drop the Category Favored column and some MA terms so to try to fit the model again.

```{r,fig.width=10, warning=F}
regressors <- regressors[,-c(2,2)]
model <- arima(random, order= c(3,0,1), xreg = regressors)
summary(model)

model <- arima(random, order= c(3,0,0), xreg = regressors)
summary(model)
```

#### Comments

- So overall model has improved with the external regressor Basket Count. But the AR(2) term has become insignificant. I am going to continue with this model.

#### Forecasting and model performance measures


```{r,fig.width=10, warning=F}
model.forecast <- predict(model, n.ahead = 10, newxreg = pr7$basket_count[c(1:10)])$pred

last.trend.value <-tail(resultdec$trend[!is.na(resultdec$trend)],10)
seasonality <- resultdec$seasonal[370:379]

forecast_normalized <- model.forecast*last.trend.value*seasonality
forecast_normalized= ts(forecast_normalized, frequency = 7, start=c(55,3))
```

```{r,fig.width=10, warning=F}
testdata <- ts(rev(testdata$sold_count), frequency = 7, start=c(55,3))

plot(testdata,ylim = c(0,100))
points(forecast_normalized,type= "l", col = 2)

```

```{r,fig.width=10, warning=F}
modelfit <- random - model$residuals

plot(random)
points(modelfit, type= "l", col = 2)

```

#### Conclusion

##### Overall looking the model, I would definetely go with an LM model as I did in the project becuase external regressors are covering the variance highly as opposed to the ARIMA model.

## PRODUCT ID: 4066298

For the product 2, I followed the same steps of product 1. Therefore, I am going to skip the descriptive comments and keep the related ones only.

### TASK 1 - Analyzing Seasonality


### 1 ) Importing and Manipulating the data



```{r}
pr8 = read.csv("alldata_item8.csv")
pr8 <- as.data.table(pr8)
pr8 <- pr8[,-c("X","w_day")]
```

```{r,warning=F}
pr8 <- mutate(pr8, event_date = ymd(event_date)) # converting event date into datetime object
pr8[, Month:=as.numeric(lubridate::month(event_date,label=F))] #adding month information as a numeric variable 
pr8[, Day:=as.numeric(lubridate::wday(event_date,label=F))] #adding day information as a numeric variable 
head(pr8)
```


```{r}
sold <- data.table(event_date =pr8$event_date,
                   sold_count = pr8$sold_count)
head(sold)
```

### Visualizations

```{r, fig.width=10}
boxplot(sold$sold_count)
```

```{r,fig.width=10}
ggplot(sold, aes(x = event_date)) + 
  geom_line(aes(y = sold_count)) +  ggtitle("Product 85004 Sold Amount") +
  xlab("Date") + ylab("Amount Sold")

```

#### ACF Plot

```{r,fig.width=10}
acf(sold$sold_count)
```

#### PACF Plot

```{r,fig.width=10}
pacf(sold$sold_count)
```

### 3 ) Decomposing the data

#### Weekly Seasonality

```{r,fig.width=10}
soldts <- ts(rev(pr8$sold_count),  freq = 7, start= c(1,1))
resultweekdec <- decompose(soldts,type= "additive")
plot(resultweekdec)
```
#### Monthly  Seasonality

```{r,fig.width=10}

soldtsmonth <- ts(rev(pr8$sold_count),  freq = 30, start= c(1,1))
resultmonthdec <- decompose(soldtsmonth,type= "additive")
plot(resultmonthdec)
```

#### Random term after decomposing weekly

```{r,fig.width=10}
plot(resultweekdec$random)
title("Random Term of Weekly Decomposed Data")
```
#### Random term after decomposing monthly

```{r,fig.width=10}
plot(resultmonthdec$random)
title("Random Term of Monthly  Decomposed Data")
```

#### Conclusion

Both the monthly and the weekly decompositon seems to have a significant seasonal component but the trend variables are not significant. We might use weekly decompositon as the random term of weekly decomposition seems more like white noise series.

```{r,fig.width=10}
random = resultweekdec$random
trend = resultweekdec$trend
season = resultweekdec$seasonal
```

### Task 2- Fitting data into ARIMA Model

#### 1 ) Deciding on the parameters

To decide on the parameters of the ARIMA model, I checked the ACF and PACF plots of the data. ACF plot has exponentially decaying and sinusodial pattern and PACF function has a sudden drop at lag 1 and no significant value after lag 1. So the model suggest a AR(1) model. But I am going to check the neighborhood to find the best model.


```{r,fig.width=10}
acf(random, na.action = na.pass)
```

```{r,fig.width=10}
pacf(random, na.action = na.pass)
```
```{r,fig.width=10}
model <- arima(random, order= c(1,0,0))
AIC(model)

model <- arima(random, order= c(2,0,0))
AIC(model)

model <- arima(random, order= c(3,0,0))
AIC(model)

model <- arima(random, order= c(4,0,0))
AIC(model)

model <- arima(random, order= c(1,0,1))
AIC(model)

model <- arima(random, order= c(2,0,1))
AIC(model)

model <- arima(random, order= c(3,0,1))
AIC(model)

model <- arima(random, order= c(4,0,1))
AIC(model)

model <- arima(random, order= c(3,0,2))
AIC(model)

model <- arima(random, order= c(2,0,2))
AIC(model)

model <- arima(random, order= c(1,0,2))
AIC(model)
```


So the best model we came up with is the one with (3,0,2) three autoregressive term and two moving average term.

#### 2 ) Fitting the Model

```{r,fig.width=10}
model <- arima(random, order= c(3,0,2))
summary(model)
```

#### Residuals after fitting

```{r,fig.width=10}
plot(model$residuals)
title("Residuals")
```

```{r,fig.width=10}
modelfit <- random - model$residuals
fitted <- modelfit+trend+season

plot(soldts)
points(fitted, type= "l", col = 2)

```
The model seems to have a good fit since zero mean assumption and constant varinace assumption seems to be held but it can be improved adding extra regressors to the model.

### Task 3 and 4 - Adding external regressors to the model

#### 1 ) Searcing for regressors

In this part, I am going to look for variables that might be correlated to sold amount and can be used as regressors in the ARIMAX model. To do this, first I am going to use pairplots. But some of the data in the columns are lost for so many data points. So only columns I can use for this, "Basket Count" , "Category Sold" , Category Visits" and "Category Favored". I checked the relation of each variable with the sold count. From the visualizations, Basket Count, Category Favored and Category Sold seems to have a correlation, so I am going to add those variables to the model.

```{r,fig.width=10, warning=F}
ggpairs(pr8, columns = c(4,7,8,10,13 ))
```

#### 2 ) Adding regressors to the ARIMA model

```{r,fig.width=10, warning=F}
traindata <- sold[-c(1:7),]
head(traindata)
testdata <- sold[c(1:7),]
head(testdata)
```

```{r,fig.width=10, warning=F}
regressors <- pr8[-c(1:7), c(7,8,13)]
head(regressors)
```

```{r,fig.width=10, warning=F}
traindatats <- ts(rev(traindata$sold_count),frequency = 7, start = c(1,1))
resultdec <- decompose(traindatats,type= "additive")
trend = resultdec$trend
season = resultdec$seasonal
random = resultdec$random 
plot(resultdec)
```
#### Without regressors

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(3,0,2))
summary(model)
```
#### With external regressor

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(3,0,2), xreg = regressors)
summary(model)
```

First AR term has lost its significancy . However I am going to forecast with this model.

#### Forecasting and model performance measures

```{r,fig.width=10, warning=F}
newregmatrix <- pr8[c(1:10), c(7,8,13)]
  
model.forecast <- predict(model, n.ahead = 10, newxreg = newregmatrix)$pred

last.trend.value <-tail(resultdec$trend[!is.na(resultdec$trend)],1)
seasonality <- resultdec$seasonal[370:379]

forecast_normalized <- model.forecast+last.trend.value+seasonality
forecast_normalized= ts(forecast_normalized, frequency = 7, start=c(55,3))
```

```{r,fig.width=10, warning=F}
testdata <- ts(rev(testdata$sold_count), frequency = 7, start=c(55,3))

plot(testdata)
points(forecast_normalized,type= "l", col = 2)

```

```{r,fig.width=10, warning=F}
modelfit <- random - model$residuals

plot(random)
points(modelfit, type= "l", col = 2)

```
```{r,28,fig.width=10, warning=F}
plot(model$residuals)

```

#### Conclusion

##### Overall looking the model, zero mean and independency of the residuals assumptions seem to be held by looking at the residuals. For this product, I might use ARIMAX model rather than LM model. But the at some points, peaks should be excluded.


## PRODUCT ID: 32939029

For the product 3, I followed the same steps of product 1. Therefore, I am going to skip the descriptive comments and keep the related ones only.

### 1 ) Importing and Manipulating the data

```{r}
pr9 = read.csv("alldata_item9.csv")
pr9 <- as.data.table(pr9)
pr9 <- pr9[,-c("X","w_day")]
```

```{r,warning=F}
pr9 <- mutate(pr9, event_date = ymd(event_date)) # converting event date into datetime object
pr9[, Month:=as.numeric(lubridate::month(event_date,label=F))] #adding month information as a numeric variable 
pr9[, Day:=as.numeric(lubridate::wday(event_date,label=F))] #adding day information as a numeric variable 
head(pr9)
```

```{r}
sold <- data.table(event_date =pr9$event_date,
                   sold_count = pr9$sold_count)
head(sold)
```

### Visualizations

```{r, fig.width=10}
boxplot(sold$sold_count)
```

```{r,fig.width=10}
ggplot(sold, aes(x = event_date)) + 
  geom_line(aes(y = sold_count)) +  ggtitle("Product 85004 Sold Amount") +
  xlab("Date") + ylab("Amount Sold")

```

#### ACF Plot

```{r,fig.width=10}
acf(sold$sold_count)
```


#### PACF Plot

```{r,fig.width=10}
pacf(sold$sold_count)
```


### 3 ) Decomposing the data

#### Weekly Seasonality

```{r,fig.width=10}
soldts <- ts(rev(pr9$sold_count),  freq = 7, start= c(1,1))
resultweekdec <- decompose(soldts,type= "additive")
plot(resultweekdec)
```

#### Monthly  Seasonality

```{r,fig.width=10}

soldtsmonth <- ts(rev(pr9$sold_count),  freq = 30, start= c(1,1))
resultmonthdec <- decompose(soldtsmonth,type= "additive")
plot(resultmonthdec)
```

#### Random term after decomposing weekly

```{r,fig.width=10}
plot(resultweekdec$random)
title("Random Term of Weekly Decomposed Data")
```

#### Random term after decomposing monthly

```{r,fig.width=10}
plot(resultmonthdec$random)
title("Random Term of Monthly  Decomposed Data")
```

#### Conclusion

- Both the monthly and the weekly decompositon seems to have a significant seasonal component but the trend variables are not significant. We might use weekly decompositon as the random term of weekly decomposition seems more like white noise series.

```{r,fig.width=10}
random = resultweekdec$random
trend = resultweekdec$trend
season = resultweekdec$seasonal
```

### Task 2- Fitting data into ARIMA Model

#### 1 ) Deciding on the parameters

- To decide on the parameters of the ARIMA model, I checked the ACF and PACF plots of the data. ACF plot has sinusodial pattern and PACF function has a negative correlation after lag 1.I am going to check the neighborhood to find the best model.

```{r,fig.width=10}
acf(random, na.action = na.pass)
```

```{r,fig.width=10}
pacf(random, na.action = na.pass)
```

```{r,fig.width=10}
model <- arima(random, order= c(1,0,0))
AIC(model)

model <- arima(random, order= c(2,0,0))
AIC(model)

model <- arima(random, order= c(3,0,0))
AIC(model)

model <- arima(random, order= c(3,0,1))
AIC(model)

model <- arima(random, order= c(3,0,2))
AIC(model)

model <- arima(random, order= c(4,0,1))
AIC(model)

model <- arima(random, order= c(2,0,2))
AIC(model)

model <- arima(random, order= c(1,0,3))
AIC(model)

model <- arima(random, order= c(1,0,2))
AIC(model)

model <- arima(random, order= c(0,0,1))
AIC(model)

model <- arima(random, order= c(0,0,2))
AIC(model)

model <- arima(random, order= c(4,0,2))
AIC(model)

model <- arima(random, order= c(2,0,1))
AIC(model)

```

So the best model we came up with is the one with (2,0,1) two autoregressive term and two moving average term.

#### 2 ) Fitting the Model

```{r,fig.width=10}
model <- arima(random, order= c(2,0,1))
summary(model)
```

#### Residuals after fitting

```{r,fig.width=10}
plot(model$residuals)
title("Residuals")
```

```{r,fig.width=10}
modelfit <- random - model$residuals
fitted <- modelfit+trend+season

plot(soldts)
points(fitted, type= "l", col = 2)

```

#### Comments

- The model seems to have a good fit since zero mean assumption and constant varinace assumption seems to be held but it can be improved adding extra regressors to the model.

### Task 3 and 4 - Adding external regressors to the model

#### 1 ) Searcing for regressors

- In this part, I am going to look for variables that might be correlated to sold amount and can be used as regressors in the ARIMAX model. To do this, first I am going to use pairplots. But some of the data in the columns are lost for so many data points. So only columns I can use for this, "Basket Count" , "Category Sold" , Category Visits" and "Category Favored". I checked the relation of each variable with the sold count. From the visualizations, Basket Count seems to have a correlation, so I am going to add that variable to the model.

#### Pair Plot

```{r,fig.width=10, warning=F}
ggpairs(pr9, columns = c(4,7,8,10,13 ))
```

#### 2 ) Adding regressors to the ARIMA model

```{r,fig.width=10, warning=F}
traindata <- sold[-c(1:7),]
head(traindata)
testdata <- sold[c(1:7),]
head(testdata)
```

```{r,fig.width=10, warning=F}
regressors <- pr9$basket_count[-c(1:7)]
head(regressors)
```

##### Now I need to decompose my train data and fit the model with and without external regressors.

```{r,fig.width=10, warning=F}
traindatats <- ts(rev(traindata$sold_count),frequency = 7, start = c(1,1))
resultdec <- decompose(traindatats,type= "additive")
trend = resultdec$trend
season = resultdec$seasonal
random = resultdec$random 
plot(resultdec)
```

#### Without regressors

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(2,0,1))
summary(model)
```

#### With external regressor

```{r,fig.width=10, warning=F}
model <- arima(random, order= c(2,0,1), xreg = regressors)
summary(model)
```

Coefficients of earlier model has not been changed, but the basket count variable seems insignificant. So I am going to continue forecasting without the external regressors.

#### Forecasting and model performance measures

```{r,fig.width=10, warning=F}
model.forecast <- predict(model, n.ahead = 10, newxreg = pr9$basket_count[c(1:10)])$pred

last.trend.value <-tail(resultdec$trend[!is.na(resultdec$trend)],10)
seasonality <- resultdec$seasonal[367:376]

forecast_normalized <- model.forecast+last.trend.value+seasonality
forecast_normalized= ts(forecast_normalized, frequency = 7, start=c(55,3))
```

```{r,fig.width=10, warning=F}
testdata <- ts(rev(testdata$sold_count), frequency = 7, start=c(55,3))

plot(testdata)
points(forecast_normalized,type= "l", col = 2)

```

```{r,fig.width=10, warning=F}

modelfit <- random - model$residuals
fitted <- modelfit+trend+season

plot(soldts)
points(fitted, type= "l", col = 2)
title("Fitted vs Real Values")


```

#### Conclusion

Overall looking the model, zero mean and independency of the residuals assumptions seem to be held by looking at the residuals but the increasing variance is kind of a problem. For this product, I might use ARIMAX model rather than LM model. But the at some points, peaks should be excluded.
